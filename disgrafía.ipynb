{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/jqjilAoe98HHrjUni1yW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5z0aiZcZmEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagenes"
      ],
      "metadata": {
        "id": "Uy1jD62VZnmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "3xWsDZaRZ8g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de los parámetros\n",
        "img_height, img_width = 128, 128  # Tamaño de las imágenes\n",
        "batch_size = 32  # Tamaño de batch\n",
        "\n",
        "# Directorio donde están las imágenes (cargar en Colab)\n",
        "train_dir = 'ruta/del/dataset/entrenamiento'\n",
        "val_dir = 'ruta/del/dataset/validacion'\n",
        "\n",
        "# Generador de imágenes para aumentar y preprocesar los datos\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,  # Normalización\n",
        "    rotation_range=20,  # Aumentación de datos\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "metadata": {
        "id": "b4-4QxNOZm1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construcción de una CNN simple\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ednuaIs5Zm--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar ResNet50 preentrenada (sin la última capa) y congelar sus pesos\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "base_model.trainable = False  # Congelar la red base\n",
        "\n",
        "# Añadir capas personalizadas encima de ResNet50\n",
        "resnet_model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history_resnet = resnet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "D9b48fJ7ZnDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Función para graficar precisión y pérdida\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Precisión\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Precisión')\n",
        "    plt.title(f'{title} - Precisión')\n",
        "    plt.legend()\n",
        "\n",
        "    # Pérdida\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "    plt.plot(history.history['val_loss'], label='Validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.title(f'{title} - Pérdida')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Graficamos para ambos modelos\n",
        "plot_history(history_cnn, \"CNN Simple\")\n",
        "plot_history(history_resnet, \"ResNet50 Transfer Learning\")\n"
      ],
      "metadata": {
        "id": "Bf0MNvcrZnFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desbloqueamos las últimas capas de ResNet50 para hacer ajuste fino\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-10]:  # Congelamos las capas iniciales\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compilación del modelo con una tasa de aprendizaje baja\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Ajuste fino del modelo\n",
        "history_resnet_finetune = resnet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "gZ_Rgt9jZnIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snPSqDhQZnRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-------------------------------------------------------**"
      ],
      "metadata": {
        "id": "EDNymCMVZ4WP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X05b3IPGYYAv"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Cargar dataset (asegúrate de subir el archivo 'disgrafia_data.csv' a tu Colab)\n",
        "df = pd.read_csv('disgrafia_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos las primeras filas del dataset\n",
        "print(df.head())\n",
        "\n",
        "# Verificamos valores faltantes y los tratamos (ejemplo: eliminamos filas con valores nulos)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Codificamos las etiquetas si hay alguna variable categórica\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])  # 'target' es la columna objetivo\n",
        "\n",
        "# Separación de características (X) y etiquetas (y)\n",
        "X = df.drop(columns=['target'])  # Asegúrate de cambiar 'target' por el nombre de tu columna objetivo\n",
        "y = df['target']\n",
        "\n",
        "# División del dataset en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalización de datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "mz3BCYijYd1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 1: SVM (Máquinas de Vectores de Soporte)"
      ],
      "metadata": {
        "id": "FTwuD0hxY7jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos un modelo SVM\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluamos el modelo SVM\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "id": "kzpzIuCAYy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 2: Random Forest"
      ],
      "metadata": {
        "id": "0s4zkwMSY_y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos un modelo Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluamos el modelo Random Forest\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "sH1NPt1VY-zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificación one-hot para la salida (si el problema es de clasificación multicategoría)\n",
        "num_classes = len(np.unique(y))\n",
        "y_train_categorical = to_categorical(y_train, num_classes)\n",
        "y_test_categorical = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Construcción de la red neuronal\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history = nn_model.fit(X_train, y_train_categorical, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluación del modelo\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test_categorical, verbose=0)\n",
        "print(\"Neural Network Accuracy:\", nn_accuracy)"
      ],
      "metadata": {
        "id": "jYv3_Z8pZJNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gráfica de precisión del modelo\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Gráfica de pérdida del modelo\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fi3XYjviZNAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTkNPhsYZMbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zSUUxT_-ZMjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}